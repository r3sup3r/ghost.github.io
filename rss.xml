<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>YΛNGΛ</title><description>No description</description><link>https://twilight.spr-aachen.com/</link><language>en</language><item><title>LoRA-based Supervised Fine-Tuning (SFT) using LLaMA Factory</title><link>https://twilight.spr-aachen.com/posts/llamafactory_supervised_fine-tuning/</link><guid isPermaLink="true">https://twilight.spr-aachen.com/posts/llamafactory_supervised_fine-tuning/</guid><description>End-to-end guide to fine-tuning an open-weight LLM with LoRA using LLaMA-Factory, merging adapters, and running the model locally.</description><pubDate>Thu, 05 Feb 2026 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Objective&lt;/h1&gt;
&lt;p&gt;This guide walks you through the process of fine-tuning an open-weight language model using LLaMA-Factory. We&apos;ll cover supervised fine-tuning with LoRA, merging the resulting adapters into a single model, and finally, running it on your local machine. This post details the entire workflow, from setting up your environment and configuring training to troubleshooting common issues, merging adapters, and executing the final model.&lt;/p&gt;
&lt;h1&gt;Setting Up Your Environment&lt;/h1&gt;
&lt;h2&gt;Linux (WSL2 / Ubuntu)&lt;/h2&gt;
&lt;p&gt;Using &lt;code&gt;WSL2&lt;/code&gt; is recommended to &lt;strong&gt;simplify GPU access&lt;/strong&gt; without the complexities of virtual machine bridging if you don&apos;t want to run things directly on Windows.&lt;/p&gt;
&lt;h2&gt;Python Virtual Environment&lt;/h2&gt;
&lt;p&gt;To keep your project dependencies isolated, create and activate a dedicated virtual environment:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;noire@Noire:~$ python3 -m venv llamafactory_sft
noire@Noire:~$ source llamafactory_sft/bin/activate
(llamafactory_sft) noire@Noire:~$
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;CUDA Installation&lt;/h2&gt;
&lt;p&gt;Before proceeding, verify that your GPU is CUDA-compatible by visiting &lt;a href=&quot;https://developer.nvidia.com/cuda-gpus&quot;&gt;NVIDIA&apos;s CUDA GPU list&lt;/a&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Download CUDA:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(llamafactory_sft) noire@Noire:~$ wget https://developer.download.nvidia.com/compute/cuda/12.2.0/local_installers/cuda_12.2.0_535.54.03_linux.run
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Run the installer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(llamafactory_sft) noire@Noire:~$ sudo sh cuda_12.2.0_535.54.03_linux.run
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://twilight.spr-aachen.com/_astro/CUDA_Installer.CEGFPXHl_bKszn.webp&quot; alt=&quot;cuda_installer&quot; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Confirm CUDA is available&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(llamafactory_sft) noire@Noire:~$ nvidia-smi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://twilight.spr-aachen.com/_astro/CUDA_available.CwkvuuBP_4NNKi.webp&quot; alt=&quot;CUDA_available.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;You can see that I am using a &lt;code&gt;Quadro T1000&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Installing LLaMA-Factory&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Clone the Repository:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(llamafactory_sft) noire@Noire:~$ git clone https://github.com/hiyouga/LLaMA-Factory.git
(llamafactory_sft) noire@Noire:~$ cd LLaMA-Factory/
(llamafactory_sft) noire@Noire:~/LLaMA-Factory$ ls
CITATION.cff  MANIFEST.in  README.md     assets  docker    pyproject.toml  scripts  tests
LICENSE       Makefile     README_zh.md  data    examples  requirements    src      tests_v1
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Install Dependencies:&lt;/strong&gt;
Navigate to the &lt;code&gt;requirements&lt;/code&gt; directory to see available dependency files.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(llamafactory_sft) noire@Noire:~/LLaMA-Factory$ ls requirements/
adam-mini.txt  aqlm.txt   bitsandbytes.txt  dev.txt   fp8-te.txt  galore.txt  hqq.txt  metrics.txt    npu.txt  sglang.txt   vllm.txt apollo.txt  badam.txt  deepspeed.txt  eetq.txt  fp8.txt  gptq.txt liger-kernel.txt  minicpm-v.txt  openmind.txt  swanlab.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, install the core requirements for LLaMA-Factory:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(llamafactory_sft) noire@Noire:~/LLaMA-Factory$ pip install -e .
(llamafactory_sft) noire@Noire:~/LLaMA-Factory$ pip install -r requirements/metrics.txt 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, let&apos;s check the version of &lt;code&gt;llamafactory&lt;/code&gt; you&apos;re running. You can do this by typing:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(llamafactory_sft) noire@Noire:~/LLaMA-Factory$ llamafactory-cli version
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, it&apos;s a good idea to look at your environment setup. Run this command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(llamafactory_sft) noire@Noire:~/LLaMA-Factory$ llamafactory-cli env
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will show you details like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- `llamafactory` version: 0.9.5.dev0
- Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39
- Python version: 3.12.3
- PyTorch version: 2.10.0+cu128 (GPU)
- Transformers version: 5.0.0
- Datasets version: 4.0.0
- Accelerate version: 1.11.0
- PEFT version: 0.18.1
- GPU type: Quadro T1000
- GPU number: 1
- GPU memory: 4.00GB
- TRL version: 0.24.0
- Git commit: 92fa3df4c4e4e338ecc1cbdc6530b118337286e3
- Default data directory: detected
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Getting Your Dataset Ready&lt;/h1&gt;
&lt;p&gt;To use your own data, you&apos;ll need to prepare it. Let&apos;s say you have a custom dataset saved as &lt;code&gt;custom_dataset.json&lt;/code&gt;. Here are a couple of examples of what that might look like:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;custom_dataset.json&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{&quot;Instruction&quot;: &quot;During initial port scanning, you find TCP 64831 running an unknown service that returns HTTP-like responses. How do you proceed?&quot;, &quot;Response&quot;: &quot;First, run service detection with nmap -sV. If it returns &apos;unknown&apos;, examine the fingerprint strings. Look for HTTP headers in the response (e.g., &apos;Location:&apos;, &apos;Set-Cookie:&apos;). This indicates a web service, possibly a Go web framework (gorilla). Add the domain to your hosts file and browse to it. Principle: Unidentified ports that return HTTP-like data are often web applications or APIs; treat them as such and enumerate endpoints and virtual hosts.&quot;}

&amp;lt;SNIP&amp;gt;

{&quot;Instruction&quot;: &quot;You discover a GoPhish instance with default credentials. After logging in, where do you look for internal domain names?&quot;, &quot;Response&quot;: &quot;Check the &apos;Email Templates&apos; section. Phishing templates contain links to fake domains (e.g., www.twitter.htb, www.paypal.htb). Extract all unique domain names. Also, check &apos;Landing Pages&apos; and &apos;Sending Profiles&apos; for more domains. Add them to your hosts file to see if they resolve to the same server. Principle: Phishing simulation platforms host fake versions of real sites; these domains often point to the same server and can reveal separate web applications or admin interfaces.&quot;}

&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Registering Your Dataset&lt;/h2&gt;
&lt;p&gt;You&apos;ll need to tell &lt;code&gt;dataset_info.json&lt;/code&gt; about your custom dataset. This file keeps track of all your local and online datasets. To include your own, you must define it and its content within &lt;code&gt;dataset_info.json&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Currently, we support datasets formatted in either Alpaca or ShareGPT style.&lt;/p&gt;
&lt;p&gt;Here&apos;s what &lt;code&gt;dataset_info.json&lt;/code&gt; might look like :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(llamafactory_sft) noire@Noire:~/LLaMA-Factory$ more data/dataset_info.json
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{
  &quot;identity&quot;: {
    &quot;file_name&quot;: &quot;identity.json&quot;
  },
  &quot;alpaca_en_demo&quot;: {
    &quot;file_name&quot;: &quot;alpaca_en_demo.json&quot; 
  },
  &quot;alpaca_zh_demo&quot;: {
    &quot;file_name&quot;: &quot;alpaca_zh_demo.json&quot;
  },
  &quot;glaive_toolcall_en_demo&quot;: {
    &quot;file_name&quot;: &quot;glaive_toolcall_en_demo.json&quot;,
    &quot;formatting&quot;: &quot;sharegpt&quot;,
    &quot;columns&quot;: {
      &quot;messages&quot;: &quot;conversations&quot;,
      &quot;tools&quot;: &quot;tools&quot;
    }
  },
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add your dataset definition, to &lt;code&gt;dataset_info.json&lt;/code&gt;. This tells &lt;strong&gt;LLaMA-Factory&lt;/strong&gt; how to map your JSON fields into the training pipeline. You should have something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(llamafactory_sft) noire@Noire:~/LLaMA-Factory$ more data/dataset_info.json
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{
  &quot;custom_dataset&quot;: {
    &quot;file_name&quot;: &quot;custom_dataset.json&quot;,
    &quot;columns&quot;: {
      &quot;instruction&quot;: &quot;Instruction&quot;,
      &quot;response&quot;: &quot;Response&quot;
    }
  },
  &quot;identity&quot;: {
    &quot;file_name&quot;: &quot;identity.json&quot;
  },
  &quot;alpaca_en_demo&quot;: {
    &quot;file_name&quot;: &quot;alpaca_en_demo.json&quot;
  },
  &quot;alpaca_zh_demo&quot;: {
    &quot;file_name&quot;: &quot;alpaca_zh_demo.json&quot;
  },
   &quot;glaive_toolcall_en_demo&quot;: {
    &quot;file_name&quot;: &quot;glaive_toolcall_en_demo.json&quot;,
    &quot;formatting&quot;: &quot;sharegpt&quot;,
    &quot;columns&quot;: {
      &quot;messages&quot;: &quot;conversations&quot;,
      &quot;tools&quot;: &quot;tools&quot;
    }
  },
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Supervised Fine-tuning&lt;/h1&gt;
&lt;p&gt;When you&apos;re looking to fine-tune a model using &lt;strong&gt;Supervised Fine-Tuning (SFT)&lt;/strong&gt; with LLaMA-Factory, you&apos;ll find helpful examples in the &lt;code&gt;examples/train_lora/&lt;/code&gt; directory. Think of these as guides rather than something to just copy and paste directly. Instead of starting from scratch, it’s best to copy an existing example and adapt it.&lt;/p&gt;
&lt;p&gt;Here is an example  in &lt;code&gt;examples/train_lora/qwen3_lora_sft.yaml&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(llamafactory_sft) noire@Noire:~/LLaMA-Factory$ cat examples/train_lora/qwen3_lora_sft.yaml
### model
model_name_or_path: Qwen/Qwen3-4B-Instruct-2507
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: lora
lora_rank: 8
lora_target: all

### dataset
dataset: identity,alpaca_en_demo
template: qwen3_nothink
cutoff_len: 2048
max_samples: 1000
preprocessing_num_workers: 16
dataloader_num_workers: 4

### output
output_dir: saves/qwen3-4b/lora/sft
logging_steps: 10
save_steps: 500
plot_loss: true
overwrite_output_dir: true
save_only_model: false
report_to: none  # choices: [none, wandb, tensorboard, swanlab, mlflow]

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
resume_from_checkpoint: null

### eval
# eval_dataset: alpaca_en_demo
# val_size: 0.1
# per_device_eval_batch_size: 1
# eval_strategy: steps
# eval_steps: 500
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I opted for the &lt;code&gt;Qwen3-1.7B&lt;/code&gt; model instead of the &lt;code&gt;4B&lt;/code&gt; versions because I ran into memory issues (Out Of Memory) during the &lt;a href=&quot;#lora-merge&quot;&gt;export&lt;/a&gt; process with the larger models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Here&apos;s the setup we used for the final training:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; We&apos;re using &lt;code&gt;Qwen/Qwen3-1.7B&lt;/code&gt; and have set &lt;code&gt;trust_remote_code&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Method:&lt;/strong&gt; This is a supervised fine-tuning (&lt;code&gt;sft&lt;/code&gt;) job using LoRA (&lt;code&gt;lora&lt;/code&gt;). We&apos;ve set the LoRA rank to 8 and are applying it to all target layers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dataset:&lt;/strong&gt; We&apos;re using a custom dataset, with the &lt;code&gt;qwen3_nothink&lt;/code&gt; template. The maximum sequence length is 2048 tokens, and we&apos;re limiting the dataset to 1000 samples. We&apos;ve allocated 16 workers for preprocessing and 4 for the data loader.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output:&lt;/strong&gt; All results will be saved to &lt;code&gt;saves/qwen3-1.7b/lora/sft&lt;/code&gt;. We&apos;ll log progress every 10 steps, save checkpoints every 500 steps, and visualize the training loss. We&apos;re also allowing the output directory to be overwritten and are not saving only the model weights. Reporting is turned off.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training:&lt;/strong&gt; We&apos;re using a batch size of 1 per device, accumulating gradients over 8 steps to simulate a larger batch. The learning rate is set to 1e-4, we&apos;re training for 3 epochs, using a cosine learning rate scheduler with a 10% warmup. We&apos;re also using &lt;code&gt;bf16&lt;/code&gt; for reduced memory usage while keeping things stable.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;### model
model_name_or_path: Qwen/Qwen3-1.7B
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: lora
lora_rank: 8
lora_target: all

### dataset
dataset: custom_dataset
template: qwen3_nothink
cutoff_len: 2048
max_samples: 1000
preprocessing_num_workers: 16
dataloader_num_workers: 4

### output
output_dir: saves/qwen3-1.7b/lora/sft
logging_steps: 10
save_steps: 500
plot_loss: true
overwrite_output_dir: true
save_only_model: false
report_to: none  # choices: [none, wandb, tensorboard, swanlab, mlflow]

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
resume_from_checkpoint: null

### eval
# eval_dataset: alpaca_en_demo
# val_size: 0.1
# per_device_eval_batch_size: 1
# eval_strategy: steps
# eval_steps: 500
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;A quick rundown of some key settings:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;model_name_or_path&lt;/code&gt;: This is either the identifier for a model on Hugging Face or the local path to your model.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;stage: sft&lt;/code&gt;: This tells the system we&apos;re doing supervised fine-tuning, which means training on instruction and response pairs.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;finetuning_type: lora&lt;/code&gt;: Instead of changing the entire model&apos;s weights, we&apos;re adding small, low-rank adapter layers.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lora_rank&lt;/code&gt;: This determines how complex the adapter layers can be. A higher rank means more capacity but also uses more VRAM.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gradient_accumulation_steps&lt;/code&gt;: This trick lets us use a larger effective batch size without needing more VRAM, by accumulating gradients over several steps.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bf16&lt;/code&gt;: This mixed-precision format helps cut down on memory usage without sacrificing too much numerical accuracy.
If you want deeper coverage, the official docs are excellent: &lt;a href=&quot;https://llamafactory.readthedocs.io/en/latest/getting_started/sft.html&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;To kick off the training, you&apos;d run:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;llamafactory-cli train qwen3_lora_sft.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;After training, you&apos;ll find these files in the output directory &lt;code&gt;saves/qwen3-1.7b/lora/sft/&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;noire@Noire:~/LLaMA-Factory$ ls saves/qwen3-1.7b/lora/sft/
README.md            adapter_model.safetensors  chat_template.jinja  tokenizer.json         train_results.json  trainer_state.json  training_loss.png
adapter_config.json  all_results.json           checkpoint-18        tokenizer_config.json  trainer_log.jsonl   training_args.bin
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;adapter_model.safetensors&lt;/code&gt; and &lt;code&gt;adapter_config.json&lt;/code&gt;: These are your trained LoRA adapters.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;training_loss.png&lt;/code&gt;: A graph showing how the loss changed during training.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;trainer_state.json&lt;/code&gt;: Contains information about the training process.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;checkpoint-18/&lt;/code&gt;: A saved state of the model at a specific point in training.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Important Note:&lt;/strong&gt; At this point, you only have the LoRA adapters. You don&apos;t have a complete, standalone model yet.&lt;/p&gt;
&lt;h2&gt;Merging the LoRA Adapters&lt;/h2&gt;
&lt;p&gt;The next step is to merge these adapters with the base model. This process combines the base model and the adapters into a single, deployable model file.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Crucial Rule:&lt;/strong&gt; Never try to merge adapters if your base model has been quantized.&lt;/p&gt;
&lt;p&gt;Always from the example directory we can find a merge configuration example &lt;code&gt;examples/merge_lora/qwen3_lora_sft.yaml&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(llamafactory_sft) noire@Noire:~/LLaMA-Factory$ cat examples/merge_lora/qwen3_lora_sft.yaml
### Note: DO NOT use quantized model or quantization_bit when merging lora adapters

### model
model_name_or_path: Qwen/Qwen3-4B-Instruct-2507
adapter_name_or_path: saves/qwen3-4b/lora/sft
template: qwen3_nothink
trust_remote_code: true

### export
export_dir: saves/qwen3_sft_merged
export_size: 5
export_device: cpu  # choices: [cpu, auto]
export_legacy_format: false
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Here&apos;s our configuration for merging:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;code&gt;Qwen/Qwen3-1.7B&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adapter:&lt;/strong&gt; &lt;code&gt;saves/qwen3-1.7b/lora/sft&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Template:&lt;/strong&gt; &lt;code&gt;qwen3_nothink&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Trust Remote Code:&lt;/strong&gt; &lt;code&gt;true&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Export Directory:&lt;/strong&gt; &lt;code&gt;merged-qwen3-1.7b&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Export Size:&lt;/strong&gt; &lt;code&gt;5&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Export Device:&lt;/strong&gt; &lt;code&gt;cpu&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Legacy Format:&lt;/strong&gt; &lt;code&gt;false&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After we copied and edited the configuration from &lt;code&gt;examples/merge_lora/qwen3_lora_sft.yaml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;file we end up with something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model_name_or_path: Qwen/Qwen3-1.7B
adapter_name_or_path: saves/qwen3-1.7b/lora/sft
template: qwen3_nothink
trust_remote_code: true

### export
export_dir: merged-qwen3-1.7b
export_size: 5
export_device: cpu  # choices: [cpu, auto]
export_legacy_format: false
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;To perform the merge, you&apos;d use:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(llamafactory_sft) noire@Noire:~/LLaMA-Factory$ lamafactory-cli export merge_config.yaml`
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;The result of the merge will be in the &lt;code&gt;merged-qwen3-1.7b/&lt;/code&gt; directory:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(llamafactory_sft) noire@Noire:~/LLaMA-Factory$ ls merged-qwen3-1.7b/
Modelfile  chat_template.jinja  config.json  generation_config.json  model.safetensors  tokenizer.json  tokenizer_config.json
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://twilight.spr-aachen.com/_astro/ollamamodelfile.DtBB3rKI_Z79GpJ.webp&quot; alt=&quot;ollamamodelfile&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Now you have a fully merged, ready-to-use model and you can notice that LLaMA-Factory even generated an &lt;strong&gt;Ollama&lt;/strong&gt; &lt;code&gt;Modelfile&lt;/code&gt;  which can be run with &lt;code&gt;ollama&lt;/code&gt; but this is a topic for another time.&lt;/p&gt;
&lt;h2&gt;Running the Model Locally&lt;/h2&gt;
&lt;p&gt;You can start interacting with your newly merged model by running:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(llamafactory_sft) noire@Noire:~/LLaMA-Factory$ llamafactory-cli chat --model_name_or_path merged-qwen3-1.7b
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once merged, your model will function just like any other standard model you might load.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Thinking porcess&lt;/strong&gt;
&lt;img src=&quot;https://twilight.spr-aachen.com/_astro/thoughtProcess.DhlQAk13_1BGQyb.webp&quot; alt=&quot;thought&quot; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Answer&lt;/strong&gt;
&lt;img src=&quot;https://twilight.spr-aachen.com/_astro/answerNmap.BwPoACwu_WQGNj.webp&quot; alt=&quot;answer&quot; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://llamafactory.readthedocs.io/en/latest/index.html&quot;&gt;llamafactory offocial documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Feedback&lt;/h1&gt;
&lt;p&gt;If you notice any errors or have suggestions for improvement, please reach out. Constructive feedback is always welcome and helps keep this content accurate and useful.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You can also check the project assosiated to this post at &lt;a href=&quot;https://github.com/r3sup3r/lazyllama/tree/master&quot;&gt;lazyLLaMA&lt;/a&gt; which is a tool to automatize this whole process.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I appreciate you taking the time to read this!&lt;/p&gt;
</content:encoded></item><item><title>Huntress CTF 2024</title><link>https://twilight.spr-aachen.com/posts/huntress2024/</link><guid isPermaLink="true">https://twilight.spr-aachen.com/posts/huntress2024/</guid><description>Huntress CTF 2024</description><pubDate>Wed, 02 Oct 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Huntress CTF 2024&lt;/h2&gt;
&lt;p&gt;This was my first time participating in a real Capture The Flag (CTF) event, and I’m genuinely proud of how far I was able to get. While I collaborated with a great team of colleagues, I made sure to independently tackle and document the challenges I worked on. This write-up highlights the solutions I personally contributed to, along with what I learned along the way.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;Challenge Overview&lt;/h2&gt;
&lt;p&gt;I competed as &lt;strong&gt;reSUper&lt;/strong&gt; in the CTF with my team &lt;strong&gt;OffsecThink&lt;/strong&gt;. I scored &lt;strong&gt;2753 points&lt;/strong&gt;, contributing to our total of &lt;strong&gt;5092 points&lt;/strong&gt;, and we placed &lt;strong&gt;168th&lt;/strong&gt; out of &lt;strong&gt;3447 teams&lt;/strong&gt;. Not bad for a first run. I learned a lot and had a great time!&lt;/p&gt;
&lt;h3&gt;Tables and Graphs&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Add description here&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://twilight.spr-aachen.com/_astro/image.B56IKyFG_1iipAa.webp&quot; alt=&quot;leaderboard&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;https://twilight.spr-aachen.com/_astro/images.BESImv-8_Z1nLEC2.webp&quot; alt=&quot;solves&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;https://twilight.spr-aachen.com/_astro/ScoreOverTime.CAPpYSCC_Lm63x.webp&quot; alt=&quot;score over time&quot; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;h3&gt;Solves by category&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;This section lists the challenges I solved, grouped by category. It gives a quick overview of the areas I focused on during the CTF, from reverse engineering and malware analysis to web, forensics, and warmups. Some were tough, some were fun, but all of them taught me something new.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Challenge&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#stack-it&quot;&gt;Stack it&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Reverse Engineering&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#knights-quest&quot;&gt;Knight&apos;s Quest&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Reverse Engineering&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#ocean-locust&quot;&gt;Ocean Locust&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Reverse Engineering&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#rustline&quot;&gt;Rustline&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Malware&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#eepy&quot;&gt;Eepy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Malware&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#mimi&quot;&gt;Mimi&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Malware&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#discount-programming-device&quot;&gt;Discount Programming Device&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Malware&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#ancient-fossil&quot;&gt;Ancient Fossil&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Forensics&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#keyboard-junkie&quot;&gt;Keyboard Junkie&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Forensics&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#obfuscation-station&quot;&gt;Obfuscation Station&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Forensics&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#helpfuldesk&quot;&gt;HelpfulDesk&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Web&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#linux-basics&quot;&gt;Linux Basics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Miscellaneous&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#malibu&quot;&gt;Malibu&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Miscellaneous&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#typo&quot;&gt;Typo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Warmups&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#finders-fee&quot;&gt;Finders Fee&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Warmups&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#i-cant-ssh&quot;&gt;I Can&apos;t SSH&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Warmups&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#txt-message&quot;&gt;TXT Message&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Warmups&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;#cattle&quot;&gt;Cattle&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Warmups&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr /&gt;
&lt;h2&gt;Writeups&lt;/h2&gt;
&lt;h3&gt;Stack it&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://twilight.spr-aachen.com/_astro/description.D-azPygY_Z14AQq4.webp&quot; alt=&quot;stack it description&quot; /&gt;&lt;/p&gt;
&lt;p&gt;First I Enumerate the sample.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Checking the file format and any embedded files.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;┌──(kali㉿kali)-[~/stackit]
└─$ file stack_it.bin
stack_it.bin: ELF 32-bit LSB executable, Intel 80386, version 1 (SYSV), statically linked, stripped

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;┌──(kali㉿kali)-[~/stackit]
└─$ binwalk stack_it.bin

DECIMAL       HEXADECIMAL     DESCRIPTION
--------------------------------------------------------------------------------
0             0x0             ELF, 32-bit LSB executable, Intel 80386, version 1 (SYSV)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The sample is a 32-bit Linux Executable Format file. All necessary libraries are statically linked into the binary. It is stripped of debug information and contains no embedded files.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Checking for interesting strings and inspecting the hexdump.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;┌──(kali㉿kali)-[~/stackit]
└─$ strings stack_it.bin
Hello, World!
SQQUR^V
1ecff8bece9486287dc76521a84bb7c0 # Interesting!
.shstrtab
.text
.data
.bss

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;┌──(kali㉿kali)-[~/stackit]
└─$ xxd stack_it.bin
00000000: 7f45 4c46 0101 0100 0000 0000 0000 0000  .ELF............ # Elf format (7f 45 4c 46)
00000010: 0200 0300 0100 0000 0090 0408 3400 0000  ............4...
00000020: 6c20 0000 0000 0000 3400 2000 0300 2800  l ......4. ...(.
00000030: 0500 0400 0100 0000 0000 0000 0080 0408  ................
00000040: 0080 0408 9400 0000 9400 0000 0400 0000  ................
00000050: 0010 0000 0100 0000 0010 0000 0090 0408  ................
00000060: 0090 0408 7d00 0000 7d00 0000 0500 0000  ....}...}.......
00000070: 0010 0000 0100 0000 0020 0000 00a0 0408  ......... ......
00000080: 00a0 0408 4e00 0000 7400 0000 0600 0000  ....N...t.......
*
00001000: c605 50a0 0408 66c6 0551 a004 086c c605  ..P...f..Q...l..
00001010: 52a0 0408 61c6 0553 a004 0867 c605 54a0  R...a..S...g..T.
00001020: 0408 7b8d 350e a004 088d 3d2e a004 088d  ..{.5.....=.....
00001030: 1555 a004 08b9 2000 0000 8a06 3207 8802  .U.... .....2...
00001040: 4647 42e2 f5c6 027d 42c6 0200 8d15 50a0  FGB....}B.....P.
00001050: 0408 b924 0000 00ff 3283 c204 e2f9 ba0d  ...$....2.......
00001060: 0000 00b9 00a0 0408 bb01 0000 00b8 0400  ................
00001070: 0000 cd80 b801 0000 0031 dbcd 8000 0000  .........1......
*
00002000: 4865 6c6c 6f2c 2057 6f72 6c64 2100 5351  Hello, World!.SQ # Hello, World!
00002010: 5155 525e 5607 0104 0d02 0003 565b 0f50  QUR^V.......V[.P # SQQUR^V
00002020: 0701 5350 0b50 5500 515b 0106 5306 3165  ..SP.PU.Q[..S.1e # 1ecff8bece9486287dc76521a84bb7c0
00002030: 6366 6638 6265 6365 3934 3836 3238 3764  cff8bece9486287d
00002040: 6337 3635 3231 6138 3462 6237 6330 002e  c76521a84bb7c0..
00002050: 7368 7374 7274 6162 002e 7465 7874 002e  shstrtab..text.. # .shstrtab .text
00002060: 6461 7461 002e 6273 7300 0000 0000 0000  data..bss....... # .data .bss
*
00002090: 0000 0000 0b00 0000 0100 0000 0600 0000  ................
000020a0: 0090 0408 0010 0000 7d00 0000 0000 0000  ........}.......
000020b0: 0000 0000 1000 0000 0000 0000 1100 0000  ................
000020c0: 0100 0000 0300 0000 00a0 0408 0020 0000  ............. ..
000020d0: 4e00 0000 0000 0000 0000 0000 0400 0000  N...............
000020e0: 0000 0000 1700 0000 0800 0000 0300 0000  ................
000020f0: 50a0 0408 4e20 0000 2400 0000 0000 0000  P...N ..$.......
00002100: 0000 0000 0400 0000 0000 0000 0100 0000  ................
00002110: 0300 0000 0000 0000 0000 0000 4e20 0000  ............N ..
00002120: 1c00 0000 0000 0000 0000 0000 0100 0000  ................
00002130: 0000 0000                                ....

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After enumerating the sample, I execute the binary to observe its runtime behavior and understand what it does.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;┌──(kali㉿kali)-[~/stackit]
└─$ chmod +x stack_it.bin

┌──(kali㉿kali)-[~/stackit]
└─$ ./stack_it.bin
Hello, World!

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output of the binary execution is simply the string &quot;Hello, World!&quot;. At this point, to better understand the purpose of the embedded strings; particularly &quot;SQQUR^V&quot; and &quot;1ecff8bece9486287dc76521a84bb7c0&quot; I’ll analyze the binary’s execution flow using IDA Pro.&lt;/p&gt;
&lt;h3&gt;IDA&lt;/h3&gt;
&lt;p&gt;In IDA’s graph view, I can clearly see that the first section of this function is responsible for initializing a string at memory address 0x804A050 with the value flag{&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mov ds:byte_804A050, 66h ; &apos;f&apos;
mov ds:byte_804A051, 6Ch ; &apos;l&apos;
mov ds:byte_804A052, 61h ; &apos;a&apos;
mov ds:byte_804A053, 67h ; &apos;g&apos;
mov ds:byte_804A054, 7Bh ; &apos;{&apos;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the following instructions of this initial section, data from unk_804A00E and unk_804A02E are loaded into the source and destination buffers, pointed to by esi and edi respectively. The ecx register is set to 0x20 (32 bytes), which acts as the loop counter for the upcoming XOR operation.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lea edi, unk_804A00E
lea esi, unk_804A02E
mov ecx, 20h ; 32 bytes
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In IDA’s data view, I observe that the memory region between unk_804A00E and unk_804A02E spans exactly 32 bytes, aligning with the loop’s counter (ecx = 0x20).&lt;/p&gt;
&lt;p&gt;The second section of the function contains the loop that performs the XOR decryption. It operates byte-by-byte over two buffers:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;A byte is read from the input buffer pointed to by esi (unk_804A02E),

It is XORed with a corresponding byte from the key buffer pointed to by edi (unk_804A00E),

The result is stored in the output buffer at edx (starting at 0x804A055), which is right after the mov ds:byte_804A054, 7Bh ; &apos;{&apos; instruction meaning the XORed bytes are written in memory after the flag{ string.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After each iteration, the pointers are incremented to the next byte, and the counter in ecx is decremented. The loop continues for 32 bytes, as defined by the initial value of ecx, effectively performing a simple XOR decryption operation.&lt;/p&gt;
&lt;p&gt;loc_804903A:
mov     al, [esi]       ; Load byte from input
xor     al, [edi]       ; XOR with key
mov     [edx], al       ; Store result in output
inc     esi             ; Increments position to next byte (add esi, 1)
inc     edi
inc     edx
loop    loc_804903A     ; Decrease ecx, loop if not zero&lt;/p&gt;
&lt;p&gt;The third part of the code completes the construction of the flag. It concatenates three components:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;The prefix string flag{, stored at address 0x804A050,

The decrypted 32-byte content stored right after, starting at 0x804A055,

The closing brace } appended at the end.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This forms the full flag string in memory.&lt;/p&gt;
&lt;p&gt;At this point, further analysis of the binary isn’t necessary, as I already have everything I need: the encrypted flag and the corresponding XOR key to decrypt it. To perform the decryption, I use CyberChef
. But first, I need to extract the hex values of both the encrypted content and the key. For this purpose, I use ImHex&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/stackit]
└─$ imhex stack_it.bin&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Hex values:

    Key = 53 51 51 55 52 5E 56 07 01 04 0D 02 00 03 56 5B 0F 50 07 01 53 50 0B 50 55 00 51 5B 01 06 53

    Encrypted Flag = 31 65 63 66 66 38 62 65 63 65 39 34 38 36 32 38 37 64 63 37 36 35 32 31 61 38 34 62 62 37 63 30

Flag: flag{b4234f4bba4685dc84d6ee9a48e9c10c}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Knights Quest&lt;/p&gt;
&lt;p&gt;Add description here&lt;/p&gt;
&lt;p&gt;Ocean Locust&lt;/p&gt;
&lt;p&gt;Add description here
Rustline&lt;/p&gt;
&lt;p&gt;Add description here
Eepy&lt;/p&gt;
&lt;p&gt;Add description here
Mimi&lt;/p&gt;
&lt;p&gt;Add description here&lt;/p&gt;
&lt;p&gt;For this challenge I started by enumerating the file.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/mimi]
└─$ file mimi
mimi: Mini DuMP crash report, 18 streams, Tue Sep 10 02:33:22 2024, 0x461826 type&lt;/p&gt;
&lt;p&gt;Judged by this output we can confirm that the file is a memory dump of a crash report, a Mini DuMP crash report. So I Googled what was a Mini DuMP crash report.&lt;/p&gt;
&lt;p&gt;Knowing that I continued with my information gathering.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/mimi]
└─$ strings mimi &amp;gt; mimi_strings.txt&lt;/p&gt;
&lt;p&gt;After a while looking for interesting strings in the output, I found a lsass.pdb string, which is a clear reference to a lsass process, which is a crucial process as hinted in the description.&lt;/p&gt;
&lt;p&gt;To dump the lsass db I used pypykatz being on Linux.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/…/offensive/CTFs/huntress2024/mimi]
└─$ pypykatz lsa minidump mimi
INFO:pypykatz:Parsing file mimi
FILE: ======== mimi =======
== LogonSession ==
authentication_id 709786 (ad49a)
session_id 1
username mimi
domainname windows11
logon_server WINDOWS11
logon_time 2024-09-10T02:32:50.802254+00:00
sid S-1-5-21-940291183-874774319-2012240919-1002
luid 709786
== MSV ==
Username: mimi
Domain: windows11
LM: NA
NT: 5e088b316cc30d7b2d0158cb4bd9497c
SHA1: c1bd67cf651fdbcf27fd155f488721f52fff64fa
DPAPI: c1bd67cf651fdbcf27fd155f488721f52fff64fa
== WDIGEST [ad49a]==
username mimi
domainname windows11
password flag{7a565a86761a2b89524bf7bb0d19bcea} # Here is the flag!!!
password (hex)66006c00610067007b00370061003500360035006100380036003700360031006100320062003800390035003200340062006600370062006200300064003100390062006300650061007d0000000000&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  ...

Flag: flag{7a565a86761a2b89524bf7bb0d19bcea}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Discount Programming Device&lt;/p&gt;
&lt;p&gt;Add description here&lt;/p&gt;
&lt;p&gt;Checkign for file format&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/…/offensive/CTFs/huntress2024/DiscountProgrammingDevice]
└─$ file deof.py
deof.py: ASCII text, with very long lines (4093)&lt;/p&gt;
&lt;p&gt;The file extension is .py but the file format indicates that the file is in reality ASCII text. So I checked the content of the file.&lt;/p&gt;
&lt;h1&gt;(content omitted in original snippet)&lt;/h1&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/DiscountProgrammingDevice]
└─$ python oops.py
flag{2543ff1e714bC2eb9ff78128232785ad}&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Flag: flag{2543ff1e714bC2eb9ff78128232785ad}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ancient Fossil&lt;/p&gt;
&lt;p&gt;For this challenge we are given a file ancient.fossil and we have to find the flag analysing it.&lt;/p&gt;
&lt;p&gt;The sample is a .fossil file, an extension I hadn&apos;t heard of at the time of the challenge. So I looked for information online, I found this interesting website
. But before I start reading the entire thing, I decided to check that the file is actually a fossil format file.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/Ancienfocil]
└─$ file ancient.fossil
ancient.fossil: SQLite 3.x database (Fossil repository), last written using SQLite version 3046000, file counter 560, database pages 158, cookie 0x2b, schema 4, UTF-8, version-valid-for 560&lt;/p&gt;
&lt;p&gt;The output was quite confusing for me, as it presented the file as a SQLite database. But still was indicating it was a Fossil repository, so I concluded that this .fossil file can be manipulated with sqlite. So I look into the file with sqlite3, first I searched for tables in the database.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/Ancienfocil]
└─$ sqlite3 ancient.fossil
SQLite version 3.46.1 2024-08-13 09:16:08
Enter &quot;.help&quot; for usage hints.
sqlite&amp;gt; SELECT name FROM sqlite_master WHERE type=&apos;table&apos;;
blob
delta
rcvfrom
user
config
shun
private
reportfmt
concealed
filename
mlink
plink
leaf
event
phantom
orphan
unclustered
unsent
tag
tagxref
backlink
attachment
ticket
ticketchng
cherrypick
sqlite_stat1
chat
sqlite_sequence
admin_log
accesslog&lt;/p&gt;
&lt;p&gt;That is quite a lot of tables. After going through the various tables I didn&apos;t find anything interesting, at first. So I went back the fossil website and read some more about the fossil format file, it is at this point that I stumbled upon these information.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Each artifact in the repository is named by a hash of its content. No prefixes, suffixes, or other information is added to an artifact before the hash is computed. The artifact name is just the (lower-case hexadecimal) hash of the raw artifact.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;In the current implementation (as of 2017-02-27) the artifacts that make up a fossil repository are stored as delta- and zlib-compressed blobs in an SQLite database.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first two tables in the SQLite were blob and delta, so I went back the db and checked again to have a better look on the content of these tables.&lt;/p&gt;
&lt;p&gt;sqlite&amp;gt; select * from blob; # blob have multiple entries, 604.
1|1|163|a6df33fb5e4fb160ad3a1611d4b0d05124e27b6a36a3197e7bbe5d6a0cbcc554|
2|2|45|8ab2f4f61d77e3a9e76c30b747d02a2d2d01015f00eba889c3705601b8940982|
3|2|329|47fcb20ec9819bec1d5081e04b902d46d26a96db7715a235fd5bbaa5ef8a1400|
*
602|402|45|66ee82da30288341f1ef1b2564067b6887a90e959439c267abd1f743c646fac6|
603|402|329|6f14e52d986d2b2645556b2d6ab8f9fa15c378eb9d578d79d3ffb9abd01f1292|
604|403|217|e410648e20a745481f21e0e20bcfe35acbbfe831c05258f639b948a9f02fb098|&lt;/p&gt;
&lt;p&gt;sqlite&amp;gt; select * from delta; # delta was empty&lt;/p&gt;
&lt;p&gt;So what do I know? I know that the artifacts are stored in a blob and that &quot;The artifact name is just the (lower-case hexadecimal) hash of the raw artifact&quot;. At this point, I copied the contents of the &quot;blob&quot; table into a .txt file making sure of taking only the fourth column which represents the name of the artifact, then I searched for the string flag without success...&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/Ancienfocil]
└─$ cat  hashed.txt  | cut -d&quot;|&quot; -f4 &amp;gt; artifacts.txt&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/Ancienfocil]
└─$ cat artifacts.txt
a6df33fb5e4fb160ad3a1611d4b0d05124e27b6a36a3197e7bbe5d6a0cbcc554
8ab2f4f61d77e3a9e76c30b747d02a2d2d01015f00eba889c3705601b8940982
47fcb20ec9819bec1d5081e04b902d46d26a96db7715a235fd5bbaa5ef8a1400
*
66ee82da30288341f1ef1b2564067b6887a90e959439c267abd1f743c646fac6
6f14e52d986d2b2645556b2d6ab8f9fa15c378eb9d578d79d3ffb9abd01f1292
e410648e20a745481f21e0e20bcfe35acbbfe831c05258f639b948a9f02fb098&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/Ancienfocil]
└─$ cat artifacts.txt | grep flag&lt;/p&gt;
&lt;h1&gt;Empty, no flag&lt;/h1&gt;
&lt;p&gt;Of course, these are random hash values.&lt;/p&gt;
&lt;p&gt;I then returned to the Fossil website to read more about the file format. That’s when I came across the download
page, where I discovered a tool called Fossil, so I downloaded it.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/Ancienfocil]
└─$ ./fossil
Usage: ./fossil COMMAND ...
or: ./fossil help           -- for a list of common commands
or: ./fossil help COMMAND   -- for help with the named command&lt;/p&gt;
&lt;p&gt;Commands and filenames may be passed on to fossil from a file
by using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./fossil --args FILENAME ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each line of the file is assumed to be a filename unless it starts
with &apos;-&apos; and contains a space, in which case it is assumed to be
another flag and is treated as such. --args FILENAME may be used
in conjunction with any other flags.&lt;/p&gt;
&lt;p&gt;Next I checked for the help.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/Ancienfocil]
└─$ ./fossil help
Usage: ./fossil help TOPIC
Things to try:&lt;/p&gt;
&lt;p&gt;./fossil help help
./fossil help -o
./fossil help -a
./fossil search -h TOPIC&lt;/p&gt;
&lt;p&gt;Other common values for TOPIC:&lt;/p&gt;
&lt;p&gt;add          chat         fdiff        merge-info   rm           ui
addremove    cherrypick   finfo        mv           settings     undo
all          clean        gdiff        open         sql          unversioned
amend        clone        grep         patch        ssl-config   update
annotate     commit       help         pull         stash        version
bisect       dbstat       info         push         status       xdiff
blame        delete       init         rebuild      sync
branch       describe     ls           remote       tag
cat          diff         merge        repack       timeline
changes      extras       merge-base   revert       tree
This is fossil version 2.26 [dfc0f1b41f] 2025-03-13 10:34:34 UTC&lt;/p&gt;
&lt;p&gt;I tried some of the commands, but were not very helpful until I tried the open command.&lt;/p&gt;
&lt;p&gt;It creates a .fslckout file on the current directory. The help of the open command reads like follow.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/Ancienfocil]
└─$ ./fossil open -help
Usage: fossil open REPOSITORY ?VERSION? ?OPTIONS?&lt;/p&gt;
&lt;p&gt;Open a new connection to the repository name REPOSITORY.  A check-out
for the repository is created with its root at the current working
directory, or in DIR if the &quot;--workdir DIR&quot; is used.
*&lt;/p&gt;
&lt;p&gt;Then I tried the ui command.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/Ancienfocil]
└─$ ./fossil ui ancient.fossil
Listening for HTTP requests on TCP port 8080&lt;/p&gt;
&lt;p&gt;Which opened a User Interface on localhost:8080. Looking for the menu entries I found a List of Artifacts entry.&lt;/p&gt;
&lt;p&gt;That menu redirected me to the localhost:8080/bloblist page, which appears to list all the artifacts in the repository. Judging by the table’s content, these artifacts correspond to the entries found in the blob table when viewed through SQLite.&lt;/p&gt;
&lt;p&gt;After digging further into the help option, I discovered the artifact command. Using it with the -a option displayed the full list of available commands, the most interesting was the artifact option.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/Ancienfocil]
└─$ ./fossil help -a
3-way-merge    clone          hook           reconstruct    sync
add            close          http           redo           tag
addremove      commit         import         remote         tarball
alerts         configuration  info           remote-url     ticket
all            dbstat         init           rename         timeline
amend          deconstruct    interwiki      repack         tls-config
annotate       delete         leaves         reparent       touch
artifact       descendants    login-group    revert         tree
*
checkout       grep           purge          ssl-config
cherrypick     hash-policy    push           stash
clean          help           rebuild        status&lt;/p&gt;
&lt;p&gt;So I tried it to query an artifact.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/Ancienfocil]
└─$ ./fossil artifact a6df33fb5e4fb160ad3a1611d4b0d05124e27b6a36a3197e7bbe5d6a0cbcc554
C initial\sempty\scheck-in
D 2024-10-16T20:57:31.743
R d41d8cd98f00b204e9800998ecf8427e
T *branch * trunk
T *sym-trunk *
U kali
Z ffb27377fbc9243f5f3d06ee44d2ac33&lt;/p&gt;
&lt;p&gt;Now with the rest of the artifacts and grep for flag.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/Ancienfocil]
└─$ for a in $(cat artifacts.txt); do fossil artifact $a; done | grep flag&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Flag: flag{2ed33f365669ea9f10b1a4ea4566fe8c}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keyboard Junkie&lt;/p&gt;
&lt;p&gt;Add description here&lt;/p&gt;
&lt;p&gt;Checking the file format&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/keyboardjunkie]
└─$ file keyboard_junkie
keyboard_junkie: pcap capture file, microsecond ts (little-endian) - version 2.4 (Memory-mapped Linux USB, capture length 245824)&lt;/p&gt;
&lt;p&gt;It is a .pcap capture file, so I opened it in Wireshark.&lt;/p&gt;
&lt;p&gt;wireshark keyboard_junkie&lt;/p&gt;
&lt;p&gt;After a quick walk on the traffic logs, I deducted that the intercepted traffic was from a USB (see the protocol) device to the host, probably a keyboard given the challenge description. That was a first for me, so I googled it, here
and then I basically found the solution to this challenge here
. I then followed the instructions written in these articles, keeping what worked for me.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/keyboardjunkie]
└─$ tshark -r keyboard_junkie -T fields -e usb.capdata &amp;gt; lefovers.txt&lt;/p&gt;
&lt;p&gt;The output was a bit messed up with spaces between lines.&lt;/p&gt;
&lt;p&gt;So I made a quick cleaning.&lt;/p&gt;
&lt;p&gt;sed ‘/$s/d’ -i leftovers.txt&lt;/p&gt;
&lt;p&gt;Then I just had to run the script and report the flag, using the code I found earlier in this ctf writeup
.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/keyboardjunkie]
└─$ cat mapstroke.py
newmap = {
2: &quot;PostFail&quot;,
4: &quot;a&quot;, 5: &quot;b&quot;, 6: &quot;c&quot;, 7: &quot;d&quot;, 8: &quot;e&quot;, 9: &quot;f&quot;,
10: &quot;g&quot;, 11: &quot;h&quot;, 12: &quot;i&quot;, 13: &quot;j&quot;, 14: &quot;k&quot;, 15: &quot;l&quot;, 16: &quot;m&quot;,
17: &quot;n&quot;, 18: &quot;o&quot;, 19: &quot;p&quot;, 20: &quot;q&quot;, 21: &quot;r&quot;, 22: &quot;s&quot;, 23: &quot;t&quot;,
24: &quot;u&quot;, 25: &quot;v&quot;, 26: &quot;w&quot;, 27: &quot;x&quot;, 28: &quot;y&quot;, 29: &quot;z&quot;, 30: &quot;1&quot;,
31: &quot;2&quot;, 32: &quot;3&quot;, 33: &quot;4&quot;, 34: &quot;5&quot;, 35: &quot;6&quot;, 36: &quot;7&quot;, 37: &quot;8&quot;,
38: &quot;9&quot;, 39: &quot;0&quot;, 40: &quot;Enter&quot;, 41: &quot;esc&quot;, 42: &quot;del&quot;, 43: &quot;tab&quot;,
44: &quot;space&quot;, 45: &quot;-&quot;, 47: &quot;[&quot;, 48: &quot;]&quot;, 56: &quot;/&quot;, 57: &quot;CapsLock&quot;,
79: &quot;RightArrow&quot;, 80: &quot;LeftArrow&quot;
}&lt;/p&gt;
&lt;p&gt;with open(&apos;leftovers.txt&apos;) as myKeys:
for i, line in enumerate(myKeys, start=1):
bytesArray = bytearray.fromhex(line.strip())&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    for byte in bytesArray:
        if byte != 0:
            keyVal = int(byte)
            if keyVal in newmap:
                print(newmap[keyVal])
            else:
                print(f&quot;No map found for this value: {keyVal}&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;python mapstroke.py&lt;/p&gt;
&lt;p&gt;The flag was in the output.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/keyboardjunkie]
└─$ python mapstroke.py
No map found for this value: 128
m
s
o
space
t
h       # the
e
space
a
n
s       # answer
w
e
r
space
i
s       # is
space
f
l
a       # flag
g
PostFail
PostFail
[
PostFail
f
7
7
3
3
e
0
0
9
3
b
7
d
2
8       # f7733e0093b7d281dd0a30fcf34a9634
1
d
d
0
a
3
0
f
c
f
3
4
a
9
6
3
4
PostFail
PostFail
]
PostFail
space
h
a
h
a       # ahahah
h
a
h
space
l
o       # lol
l
Enter
No map found for this value: 1
No map found for this value: 1
c&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Flag: flag{f7733e0093b7d281dd0a30fcf34a9634}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obfuscation Station&lt;/p&gt;
&lt;p&gt;Add description here
HelpfulDesk&lt;/p&gt;
&lt;p&gt;Add description here&lt;/p&gt;
&lt;p&gt;The challenge exposed a web service at http://challenge.ctf.games:30769. On the /Security/Bulettin page, I found logs discussing vulnerabilities that had been fixed in version 1.2 of the software. Since the current version running on the server was 1.1 (as shown at the bottom of the page), I suspected it might still be vulnerable. So, I downloaded the 1.1 update to take a closer look.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/helpfulDesk/helpfuldesk11]
└─$ ls
appsettings.Development.json                               Microsoft.CodeAnalysis.CSharp.Workspaces.resources_5.dll
appsettings.json                                           Microsoft.CodeAnalysis.CSharp.Workspaces.resources_6.dll
dotnet-aspnet-codegenerator-design.dll                     Microsoft.CodeAnalysis.CSharp.Workspaces.resources_7.dll
helpfuldesk-1.1.zip                                        Microsoft.CodeAnalysis.CSharp.Workspaces.resources_8.dll
HelpfulDesk.deps.json                                      Microsoft.CodeAnalysis.CSharp.Workspaces.resources_9.dll
HelpfulDesk.dll                                            Microsoft.CodeAnalysis.CSharp.Workspaces.resources.dll
HelpfulDesk.exe                                            Microsoft.CodeAnalysis.dll
HelpfulDesk.pdb                                            Microsoft.CodeAnalysis.Features.dll
HelpfulDesk.runtimeconfig.json                             Microsoft.CodeAnalysis.Features.resources_10.dll
HelpfulDesk.staticwebassets.runtime.json                   Microsoft.CodeAnalysis.Features.resources_11.dll
Humanizer.dll                                              Microsoft.CodeAnalysis.Features.resources_12.dll
Humanizer.resources_10.dll                                 Microsoft.CodeAnalysis.Features.resources_1.dl
stringspdb.txt&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;                                                *
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Microsoft.CodeAnalysis.CSharp.Workspaces.resources_1.dll   System.Security.Permissions.dll
Microsoft.CodeAnalysis.CSharp.Workspaces.resources_2.dll   System.Windows.Extensions_1.dll
Microsoft.CodeAnalysis.CSharp.Workspaces.resources_3.dll   System.Windows.Extensions.dll
Microsoft.CodeAnalysis.CSharp.Workspaces.resources_4.dll&lt;/p&gt;
&lt;p&gt;The directory contained a lot of dll files, but one file particularly caught my attention: HelpfulDesk.pdb. So I started my enumeration on it.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/…/CTFs/huntress2024/helpfulDesk/helpfuldesk11]
└─$ file HelpfulDesk.pdb
HelpfulDesk.pdb: Microsoft Roslyn C# debugging symbols version 1.0&lt;/p&gt;
&lt;p&gt;I wasn’t familiar with the .pdb file format, so I Googled it and found some info here
. After a bit of research, I learned that JetBrains dotPeek is a solid option for inspecting PDB files—and it worked perfectly. Before diving in, I ran the strings command to enumerate readable content. I spotted some interesting terms like userCredentials and credentialList, but nothing immediately useful. So I loaded the file into dotPeek and started digging through the code, where I found an interesting path.&lt;/p&gt;
&lt;p&gt;Following the path brought me into a setup wizard where I could set a new password to gain access to the application.&lt;/p&gt;
&lt;p&gt;On the dashboard, I found a list of several client entries, which seemed like a good place to start digging.&lt;/p&gt;
&lt;p&gt;Connecting to HOST-WIN-DX130S2 brought me to a page that contained the flag.txt file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Flag: flag{03a6f458b7483e93c37bd94b6dda462b}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Linux Basics&lt;/p&gt;
&lt;p&gt;Add description here
Malibu&lt;/p&gt;
&lt;p&gt;Establishing connection to the server. Pressing Enter twice the server answered with a 400 Bad Request.&lt;/p&gt;
&lt;p&gt;So I sent a correct HTTP request. The server response disclosed information about the running technology: MinIO server.&lt;/p&gt;
&lt;p&gt;Since I haven&apos;t seen this technology before, I googled it and found information on this MinIO
github repository. The first sentence in the README reads like follow:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MinIO is a High Performance Object Storage released under GNU Affero General Public License v3.0. It is API compatible with Amazon S3 cloud storage service.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not having experience with Amazon S3 I read the documentation
. Basically what I understood was that Amazon S3 is an object store and you store these objects in one or more buckets. So I had to find a way to enumerate the buckets on the server. For that I brute forced the server for common bucket names using this list
.&lt;/p&gt;
&lt;p&gt;for bucket in $(cat list.txt); do echo &quot;&lt;em&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;$bucket&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/em&gt;&quot;; curl -s http://challenge.ctf.games:30126/$bucket; done&lt;/p&gt;
&lt;p&gt;Until I found an interesting bucket named bucket.&lt;/p&gt;
&lt;p&gt;At some point during the challenge, I noticed a subtle hint in the description suggesting that brute-forcing wasn’t necessary. The mention of an object to bring to the beach pointed directly to a “bucket”, hinting at the use of AWS CLI.&lt;/p&gt;
&lt;p&gt;Now to access the buckets! I downloaded the bucket content on my computer to perform a local enumeration.&lt;/p&gt;
&lt;p&gt;aws s3 sync s3://bucket/ ./ --no-sign-request --endpoint-url http://challenge.ctf.games:30126&lt;/p&gt;
&lt;p&gt;The directories contained files with random strings I tried to decode for a while...&lt;/p&gt;
&lt;p&gt;... until I just decided to grep for the string flag.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Flag: flag{800e6603e86fe0a68875d3335e0daf81}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Typo&lt;/p&gt;
&lt;p&gt;Add description here&lt;/p&gt;
&lt;p&gt;Password: userpass&lt;/p&gt;
&lt;p&gt;ssh -p 30511 user@challenge.ctf.games&lt;/p&gt;
&lt;p&gt;After establishing a connection to the challenge server, I was presented with the following output.&lt;/p&gt;
&lt;p&gt;The output comes from the sl command, which is typically triggered when someone mistypes ls. This suggests that sl is executed automatically upon connection, likely configured to run whenever a new shell session starts, possibly through an entry in ~/.bashrc or ~/.zshrc. Noticing that command output was returned automatically, I connected to the server and manually ran ls via ssh to check the contents of the current directory.&lt;/p&gt;
&lt;p&gt;ssh -p 30511 user@challenge.ctf.games ls -al&lt;/p&gt;
&lt;p&gt;The flag.txt file was located in the current directory. All that was left to do was to run cat flag.txt to retrieve the flag.&lt;/p&gt;
&lt;p&gt;ssh -p 30511 user@challenge.ctf.games cat flag.txt&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Flag: flag{36a0354fbf59df454596660742bf09eb}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finders Fee&lt;/p&gt;
&lt;p&gt;Add description here&lt;/p&gt;
&lt;p&gt;This challenge involved a simple privilege escalation. Once I connected to the machine, hinted by the challenge title, I began my escalation by checking for anything that might be found by commands like grep and find, readable text files, exposed configurations, hardcoded passwords, and eventually, SUID or SGID files.&lt;/p&gt;
&lt;p&gt;find / -type f -perm /u=s,g=s -exec ls -l {} ; 2&amp;gt;/dev/null&lt;/p&gt;
&lt;p&gt;I found a /usr/bin/find binary with SGID bit set, GTFOBins!&lt;/p&gt;
&lt;p&gt;/usr/bin/find . -exec /bin/sh -p ; -quit&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Flag: flag{5da1de289823cfc200adf91d6536d914}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I Can&apos;t SSH&lt;/p&gt;
&lt;p&gt;Add description here&lt;/p&gt;
&lt;p&gt;After saving the key and setting correct permissions with:&lt;/p&gt;
&lt;p&gt;chmod 600 id_rsa&lt;/p&gt;
&lt;p&gt;I connected to the target server:&lt;/p&gt;
&lt;p&gt;ssh -i id_rsa -p 30442 user@challenge.ctf.games&lt;/p&gt;
&lt;p&gt;I ran into a libcrypto error, which usually means the private key format is either incompatible or malformed. To fix it, I copied the contents of the downloaded id_rsa and echoed it back into a new id_rsa file. Not the most elegant solution, I’m sure, but hey, it worked!&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/…/offensive/CTFs/huntress2024/icantssh]
└─$ echo &quot;-----BEGIN RSA PRIVATE KEY-----
Proc-Type: 4,ENCRYPTED
DEK-Info: AES-128-CBC,622AEAFD3F65C070D41882F436D6EC96&lt;/p&gt;
&lt;p&gt;GkSBqPbakjy92HoEBPmRqmp2Ie2hKF22GlQML+7KHNxW1OLNzevOeGLVLhJNBLwZ
CpER+eXxvpMwVaJhmgoUMpBjBdWxBHchb2BcZ40ckaliwR6oeYsOQ4QShKUvOwcU
5RZsMwvvh+ZnQiLHtst0+gIJBCs/+oFZn6vdJmn9XiGXPicuFK8OEs3ietPe+MiG
*
E4hRjTGuo7hgbjo6D8ilEpsv21g2TqcQqEzpsDj2rVB7vH266WTPgN2HAroa5NfF
8XcQjoijZW74YoWyzlVhiAZe/aO9/B8l5/uJeYjc/xEeMm6AlzQgdGKigicDsMRg
6VFRAu/1JbmJiEXsfiBnfJsW6Q3Q8WnTeH0K95UIRwgM1TvnOYTGOieq4mZsXB6M
-----END RSA PRIVATE KEY-----&quot; &amp;gt; id_rsa&lt;/p&gt;
&lt;p&gt;I generated a new RSA key using:&lt;/p&gt;
&lt;p&gt;ssh-keygen -p -f id_rsa -m PEM&lt;/p&gt;
&lt;p&gt;I was then able to access the server and retrieve the flag.&lt;/p&gt;
&lt;p&gt;ssh -i id_rsa -p 30442 user@challenge.ctf.games&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Flag: flag{eef128722ec1ce1542aa1b486dbb1361}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;TXT Message&lt;/p&gt;
&lt;p&gt;Add description here&lt;/p&gt;
&lt;p&gt;While reading the challenge, I noticed that the first two letters in the word “odd” were green. When I hovered over them, I saw they were clickable, so I clicked to see what would happen. The link redirected me to a Wikipedia page
. It turned out to be a wiki page on the od command, a nice and subtle clue.&lt;/p&gt;
&lt;p&gt;With this in mind, I began my DNS enumeration, since the challenge hinted at something hidden in the DNS records.&lt;/p&gt;
&lt;p&gt;dig ctf.games ANY&lt;/p&gt;
&lt;p&gt;I found several DNS records, but one stood out: it was a TXT record containing a series of numbers.&lt;/p&gt;
&lt;p&gt;146 154 141 147 173 061 064 145 060 067 062 146 067 060 065 144 064 065 070 070 062 064 060 061 144 061 064 061 143 065 066 062 146 144 143 060 142 175&lt;/p&gt;
&lt;p&gt;Which seemed particularly odd, just like the challenge title hinted. I figured the od command might be useful for decoding the string, given the challenge hint—but I couldn’t quite figure out how to make it work. After a few failed attempts, I realized why: od takes binary input and shows you what it looks like in various formats (octal, hex, ASCII, etc.), but it doesn’t accept octal text like 146 154 141... and turn it back into characters (man od).&lt;/p&gt;
&lt;p&gt;I decided to go back and read more about octal data and the od command on the Wikipedia page.&lt;/p&gt;
&lt;p&gt;I realized the numbers weren’t in proper octal format, so I formatted each one into a valid Python-supported octal representation before decoding them.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/cattle]
└─$ for n in $(echo &quot;146 154 141 147 173 061 064 145 060 067 062 146 067 060 065 144 064 065 070 070 062 064 060 061 144 061 064 061 143 065 066 062 146 144 143 060 142 175&quot;); do echo &quot;0o$n &quot; &amp;gt;&amp;gt; raw_octal.txt; done &amp;amp;&amp;amp; tr --delete &apos;\n&apos; &amp;lt; raw_octal.txt &amp;gt; formated_octal.txt&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/cattle]
└─$ cat formated_octal.txt
0o146 0o154 0o141 0o147 0o173 0o061 0o064 0o145 0o060 0o067 0o062 0o146 0o067 0o060 0o065 0o144 0o064 0o065 0o070 0o070 0o062 0o064 0o060 0o061 0o144 0o061 0o064 0o061 0o143 0o065 0o066 0o062 0o146 0o144 0o143 0o060 0o142 0o175&lt;/p&gt;
&lt;p&gt;So I searched for a Python script to convert octal values to ASCII, and ended up finding a helpful snippet on StackOverflow
.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/…/offensive/CTFs/huntress2024/TXTmessage]
└─$ cat octal2str.py
def octal_to_str(octal_str):
&apos;&apos;&apos;
It takes an octal string and return a string
:octal_str: octal str like &quot;110 145 154&quot;
&apos;&apos;&apos;
str_converted = &quot;&quot;
for octal_char in octal_str.split(&quot; &quot;):
str_converted += chr(int(octal_char, 8))
return str_converted&lt;/p&gt;
&lt;p&gt;print(octal_to_str(&quot;0o146 0o154 0o141 0o147 0o173 0o061 0o064 0o145 0o060 0o067 0o062 0o146 0o067 0o060 0o065 0o144 0o064 0o065 0o070 0o070 0o062 0o064 0o060 0o061 0o144 0o061 0o064 0o061 0o143 0o065 0o066 0o062 0o146 0o144 0o143 0o060 0o142 0o175&quot;))&lt;/p&gt;
&lt;p&gt;All I had to do was update the print statement with my own octal values, and that was it.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/…/offensive/CTFs/huntress2024/TXTmessage]
└─$ python octal2str.py
flag{14e072f705d45882401d141c562fdc0b}&lt;/p&gt;
&lt;p&gt;I modified the script to take octal values straight from the terminal, just to make testing different strings a bit easier. Totally unnecessary, but I did it for the giggles.&lt;/p&gt;
&lt;p&gt;import sys&lt;/p&gt;
&lt;p&gt;def octal_to_str(octal_str):
&apos;&apos;&apos;
It takes an octal string and return a string
:octal_str: octal str like &quot;110 145 154&quot;
:return: Decoded ASCII string
&apos;&apos;&apos;
str_converted = &quot;&quot;
for octal_char in octal_str.split(&quot; &quot;):
str_converted += chr(int(octal_char, 8))
return str_converted&lt;/p&gt;
&lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &quot;&lt;strong&gt;main&lt;/strong&gt;&quot;:
if len(sys.argv) != 2:
print(&quot;Usage: python script.py &quot;0o146 0o154 0o141 ...&quot;&quot;)
else:
input_str = sys.argv[1]
print(octal_to_str(input_str))&lt;/p&gt;
&lt;p&gt;octal2ascii2.0.py in action!&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/…/offensive/CTFs/huntress2024/TXTmessage]
└─$ python octal2ascii2.0.py &quot;0o146 0o154 0o141 0o147 0o173 0o061 0o064 0o145 0o060 0o067 0o062 0o146 0o067 0o060 0o065 0o144 0o064 0o065 0o070 0o070 0o062 0o064 0o060 0o061 0o144 0o061 0o064 0o061 0o143 0o065 0o066 0o062 0o146 0o144 0o143 0o060 0o142 0o175&quot;
flag{14e072f705d45882401d141c562fdc0b}&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Flag: flag{14e072f705d45882401d141c562fdc0b}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cattle&lt;/p&gt;
&lt;p&gt;Add description here&lt;/p&gt;
&lt;p&gt;First I downloaded the file and checked for the file format.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/cattle]
└─$ file cattle
cattle: ASCII text&lt;/p&gt;
&lt;p&gt;It was a simple ASCII text file, so I just used cat to view its contents.&lt;/p&gt;
&lt;p&gt;┌──(kali㉿kali)-[~/huntress2024/cattle]
└─$ cat cattle
OOO MoO MoO MoO MoO MoO MoO MoO MoO MMM moO MMM MMM moO MMM MOO MOo mOo MoO moO moo mOo
MMM moO MMM MMM moO MMM MOO MOo mOo MoO moO moo mOo MMM moO MMM MMM moO MMM MOO MOo mOo
MoO moO moo OOO moO OOO mOo mOo MMM moO MMM MOO MOo moO MoO mOo moo mOo mOo MMM moO moO
*
moO MoO mOo moo moO MoO MoO MoO MoO Moo mOo OOO moO OOO mOo mOo MMM moO MMM MOO MOo moO
MoO mOo moo mOo mOo MMM moO moO MMM MOO MOo moO MoO mOo moo mOo mOo mOo MMM moO moO moO
MMM MOO MOo moO MoO mOo moo moO MoO MoO MoO MoO MoO MoO MoO MoO MoO MoO MoO MoO MoO Moo
mOo&lt;/p&gt;
&lt;p&gt;The file contents looked pretty strange at first glance, so I Googled the first line. That led me to this post
, which explained that the text was actually written in a joke programming language called COW. After a bit more digging, I found an online JavaScript-based interpreter
. From there, I simply pasted in the contents of the file—and out came the flag.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Flag: flag{6cd6392eb609c6ae4c332ef6a321d9dd}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This was the first challenge I completed during the event—and also the final write-up in this post. Thanks for reading!&lt;/p&gt;
</content:encoded></item></channel></rss>